{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SVR Model - Cloud Resource Forecasting\n",
        "\n",
        "---\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. **Hyperparameter Tuning**: Grid search for optimal SVR parameters\n",
        "2. **Training**: Fit SVR models with high correlation features\n",
        "3. **Forecasting**: Multi-step ahead prediction (10 minutes = 20 steps)\n",
        "4. **Evaluation**: Calculate MAE, RMSE, MAPE, R² metrics\n",
        "5. **Comparison**: Save results for model comparison\n",
        "\n",
        "---\n",
        "\n",
        "**Dataset Info:**\n",
        "- Time interval: 30 seconds\n",
        "- Forecast horizon: 10 minutes (20 steps)\n",
        "- Models: 3 (memory_usage_pct, cpu_total_usage, system_load)\n",
        "- Method: SVR (Support Vector Regression) with RBF kernel\n",
        "- Feature selection: High correlation features from ETL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Models directory ready: models\n",
            "✓ Libraries imported\n",
            "✓ Model utilities loaded\n",
            "Analysis started: 2025-11-11 16:21:23\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Import model utilities\n",
        "from model_utils import (\n",
        "    save_model,\n",
        "    load_model,\n",
        "    calculate_metrics,\n",
        "    print_metrics,\n",
        "    save_results,\n",
        "    create_models_directory\n",
        ")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Display settings\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Create models directory\n",
        "create_models_directory()\n",
        "\n",
        "print(\"✓ Libraries imported\")\n",
        "print(\"✓ Model utilities loaded\")\n",
        "print(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Metadata:\n",
            "================================================================================\n",
            "\n",
            "memory_usage_pct:\n",
            "  Features: 7\n",
            "  List: ['load-15m', 'sys-context-switch-rate', 'cpu-system', 'cpu-user', 'load-5m', 'sys-mem-buffered', 'sys-mem-free']\n",
            "\n",
            "cpu_total_usage:\n",
            "  Features: 7\n",
            "  List: ['sys-context-switch-rate', 'sys-fork-rate', 'sys-interrupt-rate', 'load-15m', 'load-5m', 'disk-io-write', 'sys-mem-available']\n",
            "\n",
            "system_load:\n",
            "  Features: 2\n",
            "  List: ['load-5m', 'load-15m']\n",
            "\n",
            "================================================================================\n",
            "✓ Metadata loaded\n"
          ]
        }
      ],
      "source": [
        "# Load feature metadata\n",
        "with open('processed_data/feature_metadata.json', 'r') as f:\n",
        "    feature_metadata = json.load(f)\n",
        "\n",
        "print(\"Feature Metadata:\")\n",
        "print(\"=\"*80)\n",
        "for target, info in feature_metadata.items():\n",
        "    print(f\"\\n{target}:\")\n",
        "    print(f\"  Features: {info['n_features']}\")\n",
        "    print(f\"  List: {info['features']}\")\n",
        "\n",
        "# Target variables\n",
        "target_vars = ['memory_usage_pct', 'cpu_total_usage', 'system_load']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Metadata loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading memory_usage_pct...\n",
            "  X_train: (68599, 7)\n",
            "  X_test: (17150, 7)\n",
            "  y_train: 68,599 samples\n",
            "  y_test: 17,150 samples\n",
            "\n",
            "Loading cpu_total_usage...\n",
            "  X_train: (68599, 7)\n",
            "  X_test: (17150, 7)\n",
            "  y_train: 68,599 samples\n",
            "  y_test: 17,150 samples\n",
            "\n",
            "Loading system_load...\n",
            "  X_train: (68599, 2)\n",
            "  X_test: (17150, 2)\n",
            "  y_train: 68,599 samples\n",
            "  y_test: 17,150 samples\n",
            "\n",
            "================================================================================\n",
            "✓ All datasets loaded\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Load train/test datasets\n",
        "datasets = {}\n",
        "\n",
        "for target in target_vars:\n",
        "    print(f\"\\nLoading {target}...\")\n",
        "    \n",
        "    X_train = pd.read_csv(f'processed_data/{target}/X_train.csv')\n",
        "    X_test = pd.read_csv(f'processed_data/{target}/X_test.csv')\n",
        "    y_train = pd.read_csv(f'processed_data/{target}/y_train.csv').squeeze()\n",
        "    y_test = pd.read_csv(f'processed_data/{target}/y_test.csv').squeeze()\n",
        "    \n",
        "    datasets[target] = {\n",
        "        'X_train': X_train,\n",
        "        'X_test': X_test,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test,\n",
        "        'features': feature_metadata[target]['features']\n",
        "    }\n",
        "    \n",
        "    print(f\"  X_train: {X_train.shape}\")\n",
        "    print(f\"  X_test: {X_test.shape}\")\n",
        "    print(f\"  y_train: {len(y_train):,} samples\")\n",
        "    print(f\"  y_test: {len(y_test):,} samples\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ All datasets loaded\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. SVR Configuration & Hyperparameter Tuning\n",
        "\n",
        "SVR hyperparameters to tune:\n",
        "- **C**: Regularization parameter\n",
        "- **gamma**: Kernel coefficient  \n",
        "- **epsilon**: Epsilon-tube tolerance\n",
        "- **kernel**: RBF (Radial Basis Function)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVR Configuration:\n",
            "================================================================================\n",
            "Kernel: RBF\n",
            "Parameter grid:\n",
            "  C: [0.1, 1, 10]\n",
            "  gamma: ['scale', 0.001, 0.01, 0.1]\n",
            "  epsilon: [0.01, 0.1, 0.2]\n",
            "\n",
            "Grid search enabled: True\n",
            "Forecast horizon: 20 steps (10 minutes)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# SVR hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 0.001, 0.01, 0.1],\n",
        "    'epsilon': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Forecast horizon\n",
        "FORECAST_HORIZON = 20  # 10 minutes\n",
        "\n",
        "# Grid search configuration\n",
        "GRID_SEARCH = True  # Set to False to use default parameters\n",
        "N_JOBS = -1  # Use all CPU cores\n",
        "\n",
        "print(\"SVR Configuration:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Kernel: RBF\")\n",
        "print(f\"Parameter grid:\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "print(f\"\\nGrid search enabled: {GRID_SEARCH}\")\n",
        "print(f\"Forecast horizon: {FORECAST_HORIZON} steps (10 minutes)\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train SVR Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TRAINING SVR MODELS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Target: memory_usage_pct\n",
            "================================================================================\n",
            "Training samples: 68,599\n",
            "Features: 7\n",
            "\n",
            "Performing grid search...\n",
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
          ]
        }
      ],
      "source": [
        "# Train SVR models with optional grid search\n",
        "svr_models = {}\n",
        "training_results = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING SVR MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for target in target_vars:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Target: {target}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    X_train = datasets[target]['X_train']\n",
        "    y_train = datasets[target]['y_train']\n",
        "    \n",
        "    print(f\"Training samples: {len(y_train):,}\")\n",
        "    print(f\"Features: {len(X_train.columns)}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        if GRID_SEARCH:\n",
        "            print(\"\\nPerforming grid search...\")\n",
        "            \n",
        "            # Grid search with cross-validation\n",
        "            svr = SVR(kernel='rbf')\n",
        "            grid_search = GridSearchCV(\n",
        "                svr,\n",
        "                param_grid,\n",
        "                cv=3,\n",
        "                scoring='neg_mean_squared_error',\n",
        "                n_jobs=N_JOBS,\n",
        "                verbose=1\n",
        "            )\n",
        "            \n",
        "            grid_search.fit(X_train, y_train)\n",
        "            \n",
        "            best_model = grid_search.best_estimator_\n",
        "            best_params = grid_search.best_params_\n",
        "            best_score = -grid_search.best_score_  # Convert back to positive MSE\n",
        "            \n",
        "            print(f\"\\n✓ Grid search completed\")\n",
        "            print(f\"  Best parameters: {best_params}\")\n",
        "            print(f\"  Best CV MSE: {best_score:.6f}\")\n",
        "            \n",
        "        else:\n",
        "            print(\"\\nTraining with default parameters...\")\n",
        "            best_model = SVR(kernel='rbf', C=1.0, gamma='scale', epsilon=0.1)\n",
        "            best_model.fit(X_train, y_train)\n",
        "            best_params = {'C': 1.0, 'gamma': 'scale', 'epsilon': 0.1}\n",
        "            best_score = None\n",
        "        \n",
        "        training_time = time.time() - start_time\n",
        "        \n",
        "        # Store model\n",
        "        svr_models[target] = best_model\n",
        "        \n",
        "        # Save model using utility\n",
        "        print(\"\\nSaving model...\")\n",
        "        model_path = save_model(\n",
        "            best_model,\n",
        "            model_name='svr',\n",
        "            target=target,\n",
        "            config={\n",
        "                'kernel': 'rbf',\n",
        "                'params': best_params,\n",
        "                'n_features': len(X_train.columns),\n",
        "                'features': list(X_train.columns)\n",
        "            },\n",
        "            models_dir='models'\n",
        "        )\n",
        "        \n",
        "        training_results[target] = {\n",
        "            'params': best_params,\n",
        "            'n_features': len(X_train.columns),\n",
        "            'n_samples': len(y_train),\n",
        "            'training_time': training_time,\n",
        "            'cv_mse': best_score,\n",
        "            'model_path': model_path\n",
        "        }\n",
        "        \n",
        "        print(f\"✓ Training completed in {training_time:.2f}s\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Training failed: {str(e)}\")\n",
        "        training_results[target] = {'error': str(e), 'success': False}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Training completed and models saved\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def svr_rolling_forecast(model, X_test, y_test, horizon=20):\n",
        "    \"\"\"\n",
        "    Rolling forecast for SVR\n",
        "    Predict 'horizon' steps ahead at each time point using actual features\n",
        "    \"\"\"\n",
        "    n_test = len(y_test)\n",
        "    predictions = []\n",
        "    \n",
        "    # Can only forecast where we have enough future data\n",
        "    n_forecast_points = n_test - horizon + 1\n",
        "    \n",
        "    print(f\"  Forecasting {n_forecast_points} points with horizon={horizon}\")\n",
        "    \n",
        "    for i in range(n_forecast_points):\n",
        "        # Use features at time t to predict value at time t+horizon\n",
        "        X_current = X_test.iloc[i:i+1]\n",
        "        pred = model.predict(X_current)[0]\n",
        "        predictions.append(pred)\n",
        "        \n",
        "        if (i + 1) % 5000 == 0:\n",
        "            print(f\"    Progress: {i+1}/{n_forecast_points}\")\n",
        "    \n",
        "    # Align actual values (horizon steps ahead)\n",
        "    predictions = np.array(predictions)\n",
        "    actual = y_test.iloc[horizon:horizon+n_forecast_points].values\n",
        "    \n",
        "    return predictions, actual\n",
        "\n",
        "# Perform forecasting\n",
        "print(\"=\"*80)\n",
        "print(f\"MULTI-STEP FORECASTING (Horizon: {FORECAST_HORIZON} steps = 10 minutes)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "forecast_results = {}\n",
        "\n",
        "for target in target_vars:\n",
        "    if target not in svr_models:\n",
        "        print(f\"\\n✗ Skipping {target} - model not trained\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Target: {target}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    model = svr_models[target]\n",
        "    X_test = datasets[target]['X_test']\n",
        "    y_test = datasets[target]['y_test']\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        predictions, actual = svr_rolling_forecast(model, X_test, y_test, FORECAST_HORIZON)\n",
        "        \n",
        "        forecast_time = time.time() - start_time\n",
        "        \n",
        "        forecast_results[target] = {\n",
        "            'predictions': predictions,\n",
        "            'actual': actual,\n",
        "            'n_predictions': len(predictions),\n",
        "            'forecast_time': forecast_time,\n",
        "            'horizon': FORECAST_HORIZON\n",
        "        }\n",
        "        \n",
        "        print(f\"✓ Completed in {forecast_time:.2f}s\")\n",
        "        print(f\"  Predictions: {len(predictions):,}\")\n",
        "        print(f\"  Avg time: {forecast_time/len(predictions)*1000:.2f}ms per forecast\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Forecasting failed: {str(e)}\")\n",
        "        forecast_results[target] = {'error': str(e), 'success': False}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Forecasting completed\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics using utility function\n",
        "print(\"=\"*80)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "evaluation_results = {}\n",
        "\n",
        "for target in target_vars:\n",
        "    if target not in forecast_results or 'predictions' not in forecast_results[target]:\n",
        "        print(f\"\\n✗ Skipping {target} - no predictions\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Target: {target}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    y_true = forecast_results[target]['actual']\n",
        "    y_pred = forecast_results[target]['predictions']\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(y_true, y_pred)\n",
        "    evaluation_results[target] = metrics\n",
        "    \n",
        "    # Print formatted metrics\n",
        "    print_metrics(metrics, target)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Evaluation completed\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot predictions vs actual\n",
        "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
        "fig.suptitle('SVR: Predictions vs Actual (10-minute horizon)', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, target in enumerate(target_vars):\n",
        "    if target not in forecast_results or 'predictions' not in forecast_results[target]:\n",
        "        continue\n",
        "    \n",
        "    y_true = forecast_results[target]['actual']\n",
        "    y_pred = forecast_results[target]['predictions']\n",
        "    \n",
        "    # Plot first 500 points\n",
        "    n_plot = min(500, len(y_true))\n",
        "    \n",
        "    axes[idx].plot(y_true[:n_plot], label='Actual', alpha=0.7, linewidth=1.5)\n",
        "    axes[idx].plot(y_pred[:n_plot], label='Predicted', alpha=0.7, linewidth=1.5)\n",
        "    axes[idx].set_title(f'{target} - MAE: {evaluation_results[target][\"mae\"]:.4f}, R²: {evaluation_results[target][\"r2\"]:.4f}')\n",
        "    axes[idx].set_xlabel('Time Step')\n",
        "    axes[idx].set_ylabel('Normalized Value')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plots: Predicted vs Actual\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('SVR: Predicted vs Actual', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, target in enumerate(target_vars):\n",
        "    if target not in forecast_results or 'predictions' not in forecast_results[target]:\n",
        "        continue\n",
        "    \n",
        "    y_true = forecast_results[target]['actual']\n",
        "    y_pred = forecast_results[target]['predictions']\n",
        "    \n",
        "    axes[idx].scatter(y_true, y_pred, alpha=0.3, s=10)\n",
        "    \n",
        "    # Perfect prediction line\n",
        "    min_val = min(y_true.min(), y_pred.min())\n",
        "    max_val = max(y_true.max(), y_pred.max())\n",
        "    axes[idx].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect')\n",
        "    \n",
        "    axes[idx].set_title(f'{target}\\\\nR² = {evaluation_results[target][\"r2\"]:.4f}')\n",
        "    axes[idx].set_xlabel('Actual')\n",
        "    axes[idx].set_ylabel('Predicted')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics comparison\n",
        "metrics_df = pd.DataFrame(evaluation_results).T\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('SVR Performance Metrics', fontsize=16, fontweight='bold')\n",
        "\n",
        "metrics_df['mae'].plot(kind='bar', ax=axes[0, 0], color='skyblue')\n",
        "axes[0, 0].set_title('MAE')\n",
        "axes[0, 0].set_ylabel('MAE')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "metrics_df['rmse'].plot(kind='bar', ax=axes[0, 1], color='lightcoral')\n",
        "axes[0, 1].set_title('RMSE')\n",
        "axes[0, 1].set_ylabel('RMSE')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "metrics_df['mape'].plot(kind='bar', ax=axes[1, 0], color='lightgreen')\n",
        "axes[1, 0].set_title('MAPE')\n",
        "axes[1, 0].set_ylabel('MAPE (%)')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "metrics_df['r2'].plot(kind='bar', ax=axes[1, 1], color='plum')\n",
        "axes[1, 1].set_title('R² Score')\n",
        "axes[1, 1].set_ylabel('R²')\n",
        "axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile results\n",
        "final_results = {\n",
        "    'model': 'SVR',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'forecast_horizon': FORECAST_HORIZON,\n",
        "    'forecast_horizon_minutes': FORECAST_HORIZON * 0.5,\n",
        "    'targets': {}\n",
        "}\n",
        "\n",
        "for target in target_vars:\n",
        "    if target not in evaluation_results:\n",
        "        continue\n",
        "    \n",
        "    final_results['targets'][target] = {\n",
        "        'model_config': {\n",
        "            'kernel': 'rbf',\n",
        "            'params': training_results[target]['params'],\n",
        "            'n_features': training_results[target]['n_features']\n",
        "        },\n",
        "        'training': {\n",
        "            'samples': training_results[target]['n_samples'],\n",
        "            'time_seconds': training_results[target]['training_time'],\n",
        "            'cv_mse': training_results[target].get('cv_mse'),\n",
        "            'model_path': training_results[target]['model_path']\n",
        "        },\n",
        "        'forecasting': {\n",
        "            'n_predictions': forecast_results[target]['n_predictions'],\n",
        "            'time_seconds': forecast_results[target]['forecast_time']\n",
        "        },\n",
        "        'metrics': evaluation_results[target]\n",
        "    }\n",
        "\n",
        "# Save using utility function\n",
        "output_file = save_results(final_results, 'results_svr.json')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Model: SVR\")\n",
        "print(f\"Forecast horizon: {FORECAST_HORIZON} steps ({FORECAST_HORIZON*0.5:.1f} min)\")\n",
        "print(f\"Results saved to: {output_file}\")\n",
        "print(f\"Models saved in: models/\")\n",
        "\n",
        "print(f\"\\nMetrics Overview:\")\n",
        "for target in target_vars:\n",
        "    if target in final_results['targets']:\n",
        "        metrics = final_results['targets'][target]['metrics']\n",
        "        print(f\"\\n  {target}:\")\n",
        "        print(f\"    MAE:  {metrics['mae']:.6f}\")\n",
        "        print(f\"    RMSE: {metrics['rmse']:.6f}\")\n",
        "        print(f\"    R²:   {metrics['r2']:.6f}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test with Different Horizons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with multiple horizons\n",
        "HORIZONS_TO_TEST = [60]  # 5, 10, 20, 30 minutes\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING MULTIPLE HORIZONS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Horizons: {HORIZONS_TO_TEST} steps\")\n",
        "print(f\"Minutes: {[h*0.5 for h in HORIZONS_TO_TEST]}\")\n",
        "print()\n",
        "\n",
        "horizon_comparison = {}\n",
        "\n",
        "for target in target_vars:\n",
        "    if target not in svr_models:\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Target: {target}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    model = svr_models[target]\n",
        "    X_test = datasets[target]['X_test']\n",
        "    y_test = datasets[target]['y_test']\n",
        "    \n",
        "    horizon_comparison[target] = {}\n",
        "    \n",
        "    for horizon in HORIZONS_TO_TEST:\n",
        "        print(f\"  Testing horizon: {horizon} steps ({horizon*0.5:.1f} min)... \", end='')\n",
        "        \n",
        "        try:\n",
        "            # Forecast\n",
        "            predictions, actual = svr_rolling_forecast(model, X_test, y_test, horizon)\n",
        "            \n",
        "            # Metrics\n",
        "            metrics = calculate_metrics(actual, predictions)\n",
        "            \n",
        "            horizon_comparison[target][f\"h{horizon}\"] = {\n",
        "                'horizon': horizon,\n",
        "                'horizon_minutes': horizon * 0.5,\n",
        "                'n_predictions': len(predictions),\n",
        "                'metrics': metrics\n",
        "            }\n",
        "            \n",
        "            print(f\"MAE={metrics['mae']:.4f}, R²={metrics['r2']:.4f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Failed: {str(e)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Horizon testing completed\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison table\n",
        "print(\"\\nHORIZON COMPARISON:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for target in target_vars:\n",
        "    if target not in horizon_comparison:\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{target.upper()}:\")\n",
        "    print(f\"{'Horizon':>10} {'Minutes':>10} {'MAE':>12} {'RMSE':>12} {'R²':>12}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for h_key in sorted(horizon_comparison[target].keys(),\n",
        "                       key=lambda x: horizon_comparison[target][x]['horizon']):\n",
        "        h = horizon_comparison[target][h_key]\n",
        "        print(f\"{h['horizon']:>10} {h['horizon_minutes']:>10.1f} \"\n",
        "              f\"{h['metrics']['mae']:>12.6f} {h['metrics']['rmse']:>12.6f} \"\n",
        "              f\"{h['metrics']['r2']:>12.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize performance vs horizon\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "fig.suptitle('SVR: Performance vs Forecast Horizon', fontsize=16, fontweight='bold')\n",
        "\n",
        "metrics_to_plot = ['mae', 'rmse', 'mape', 'r2']\n",
        "titles = ['MAE', 'RMSE', 'MAPE (%)', 'R²']\n",
        "colors_list = ['skyblue', 'lightcoral', 'lightgreen']\n",
        "\n",
        "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    \n",
        "    for cidx, target in enumerate(target_vars):\n",
        "        if target not in horizon_comparison:\n",
        "            continue\n",
        "        \n",
        "        horizons = []\n",
        "        values = []\n",
        "        \n",
        "        for h_key in sorted(horizon_comparison[target].keys(),\n",
        "                           key=lambda x: horizon_comparison[target][x]['horizon']):\n",
        "            h = horizon_comparison[target][h_key]\n",
        "            horizons.append(h['horizon_minutes'])\n",
        "            values.append(h['metrics'][metric])\n",
        "        \n",
        "        ax.plot(horizons, values, marker='o', linewidth=2,\n",
        "               label=target, alpha=0.7, color=colors_list[cidx])\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Horizon (minutes)')\n",
        "    ax.set_ylabel(title)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    if metric == 'r2':\n",
        "        ax.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save horizon comparison\n",
        "horizon_results = {\n",
        "    'model': 'SVR',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'horizons_tested': HORIZONS_TO_TEST,\n",
        "    'targets': horizon_comparison\n",
        "}\n",
        "\n",
        "horizon_file = save_results(horizon_results, 'results_svr_horizon_comparison.json')\n",
        "\n",
        "print(\"\\nBest horizon by MAE:\")\n",
        "for target in target_vars:\n",
        "    if target in horizon_comparison:\n",
        "        best = min(horizon_comparison[target].items(),\n",
        "                  key=lambda x: x[1]['metrics']['mae'])\n",
        "        print(f\"  {target}: {best[1]['horizon']} steps ({best[1]['horizon_minutes']:.1f} min) \"\n",
        "              f\"- MAE: {best[1]['metrics']['mae']:.6f}\")\n",
        "\n",
        "print(f\"\\n✓ Horizon comparison saved to: {horizon_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Completed:\n",
        "\n",
        "1. ✅ **Data Loading**: Loaded preprocessed train/test data\n",
        "2. ✅ **Hyperparameter Tuning**: Grid search for optimal SVR parameters\n",
        "3. ✅ **Model Training**: SVR with RBF kernel\n",
        "4. ✅ **Model Saving**: Saved to `models/` directory\n",
        "5. ✅ **Forecasting**: 10-minute ahead predictions (20 steps)\n",
        "6. ✅ **Evaluation**: MAE, RMSE, MAPE, R² metrics\n",
        "7. ✅ **Multi-Horizon Testing**: Tested with 5, 10, 20, 30 minute horizons\n",
        "8. ✅ **Visualization**: Performance comparisons\n",
        "9. ✅ **Results Saved**: JSON files for comparison\n",
        "\n",
        "### Output Files:\n",
        "\n",
        "- `models/svr_[target]_[timestamp].pkl`: Trained SVR models\n",
        "- `results_svr.json`: Main results (20-step horizon)\n",
        "- `results_svr_horizon_comparison.json`: Multi-horizon comparison\n",
        "\n",
        "### How to Load and Test:\n",
        "\n",
        "```python\n",
        "from model_utils import load_model, calculate_metrics\n",
        "\n",
        "# Load saved SVR model\n",
        "model, metadata = load_model('models/svr_memory_usage_pct_xxx.pkl')\n",
        "\n",
        "# Predict with custom horizon\n",
        "predictions, actual = svr_rolling_forecast(model, X_test, y_test, horizon=120)\n",
        "\n",
        "# Evaluate\n",
        "metrics = calculate_metrics(actual, predictions)\n",
        "print(f\"R²: {metrics['r2']:.4f}\")\n",
        "```\n",
        "\n",
        "### Model Comparison:\n",
        "\n",
        "Now you can compare SVR vs ARIMAX:\n",
        "```python\n",
        "from model_utils import load_results, compare_results\n",
        "\n",
        "comparison = compare_results(['results_arimax.json', 'results_svr.json'])\n",
        "print(comparison)\n",
        "```\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Compare SVR vs ARIMAX performance\n",
        "- Try other kernels (linear, polynomial)\n",
        "- Implement LSTM or Prophet models\n",
        "- Ensemble methods\n",
        "\n",
        "---\n",
        "\n",
        "**All SVR models and results saved successfully!**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
