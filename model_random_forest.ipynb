{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Forest Model - Cloud Resource Forecasting\n",
        "\n",
        "---\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. **Hyperparameter Tuning**: Grid search for optimal Random Forest parameters\n",
        "2. **Training**: Fit Random Forest models with high correlation features\n",
        "3. **Forecasting**: Multi-step ahead prediction (10 minutes = 20 steps)\n",
        "4. **Evaluation**: Calculate MAE, RMSE, MAPE, R² metrics\n",
        "5. **Comparison**: Save results for model comparison\n",
        "\n",
        "---\n",
        "\n",
        "**Dataset Info:**\n",
        "- Time interval: 30 seconds\n",
        "- Forecast horizon: 10 minutes (20 steps)\n",
        "- Models: 3 (memory_usage_pct, cpu_total_usage, system_load)\n",
        "- Method: Random Forest Regressor\n",
        "- Feature selection: High correlation features from ETL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# Import model utilities\n",
        "from model_utils import (\n",
        "    save_model,\n",
        "    load_model,\n",
        "    calculate_metrics,\n",
        "    print_metrics,\n",
        "    save_results,\n",
        "    create_models_directory\n",
        ")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Display settings\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Create models directory\n",
        "create_models_directory()\n",
        "\n",
        "print(\"✓ Libraries imported\")\n",
        "print(\"✓ Model utilities loaded\")\n",
        "print(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load feature metadata\n",
        "with open('processed_data/feature_metadata.json', 'r') as f:\n",
        "    feature_metadata = json.load(f)\n",
        "\n",
        "print(\"Feature Metadata:\")\n",
        "print(\"=\"*80)\n",
        "for target, info in feature_metadata.items():\n",
        "    print(f\"\\n{target}:\")\n",
        "    print(f\"  Features: {info['n_features']}\")\n",
        "    print(f\"  List: {info['features']}\")\n",
        "\n",
        "# Target variables\n",
        "target_vars = ['memory_usage_pct', 'cpu_total_usage', 'system_load']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Metadata loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load train/test datasets\n",
        "datasets = {}\n",
        "\n",
        "for target in target_vars:\n",
        "    print(f\"\\nLoading {target}...\")\n",
        "    \n",
        "    X_train = pd.read_csv(f'processed_data/{target}/X_train.csv')\n",
        "    X_test = pd.read_csv(f'processed_data/{target}/X_test.csv')\n",
        "    y_train = pd.read_csv(f'processed_data/{target}/y_train.csv').squeeze()\n",
        "    y_test = pd.read_csv(f'processed_data/{target}/y_test.csv').squeeze()\n",
        "    \n",
        "    datasets[target] = {\n",
        "        'X_train': X_train,\n",
        "        'X_test': X_test,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test,\n",
        "        'features': feature_metadata[target]['features']\n",
        "    }\n",
        "    \n",
        "    print(f\"  X_train: {X_train.shape}\")\n",
        "    print(f\"  X_test: {X_test.shape}\")\n",
        "    print(f\"  y_train: {len(y_train):,} samples\")\n",
        "    print(f\"  y_test: {len(y_test):,} samples\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ All datasets loaded\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Random Forest Configuration & Hyperparameter Tuning\n",
        "\n",
        "Random Forest hyperparameters to tune:\n",
        "- **n_estimators**: Number of trees in the forest\n",
        "- **max_depth**: Maximum depth of trees\n",
        "- **min_samples_split**: Minimum samples required to split\n",
        "- **min_samples_leaf**: Minimum samples at leaf node\n",
        "- **max_features**: Number of features to consider for best split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Forecast horizon\n",
        "FORECAST_HORIZON = 20  # 10 minutes\n",
        "\n",
        "# Grid search configuration\n",
        "GRID_SEARCH = True  # Set to False to use default parameters\n",
        "USE_RANDOMIZED = True  # Use RandomizedSearchCV for faster search\n",
        "N_ITER = 20  # Number of parameter settings sampled\n",
        "N_JOBS = -1  # Use all CPU cores\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "print(\"Random Forest Configuration:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Parameter grid:\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "print(f\"\\nGrid search enabled: {GRID_SEARCH}\")\n",
        "print(f\"Randomized search: {USE_RANDOMIZED}\")\n",
        "if USE_RANDOMIZED:\n",
        "    print(f\"Iterations: {N_ITER}\")\n",
        "print(f\"Forecast horizon: {FORECAST_HORIZON} steps (10 minutes)\")\n",
        "print(f\"Random state: {RANDOM_STATE}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Random Forest Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest models\n",
        "rf_models = {}\n",
        "training_results = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING RANDOM FOREST MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for target in target_vars:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Target: {target}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    X_train = datasets[target]['X_train']\n",
        "    y_train = datasets[target]['y_train']\n",
        "    \n",
        "    print(f\"Training samples: {len(y_train):,}\")\n",
        "    print(f\"Features: {len(X_train.columns)}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        if GRID_SEARCH:\n",
        "            rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=N_JOBS)\n",
        "            \n",
        "            if USE_RANDOMIZED:\n",
        "                print(\"\\nPerforming randomized search...\")\n",
        "                search = RandomizedSearchCV(\n",
        "                    rf,\n",
        "                    param_grid,\n",
        "                    n_iter=N_ITER,\n",
        "                    cv=3,\n",
        "                    scoring='neg_mean_squared_error',\n",
        "                    n_jobs=N_JOBS,\n",
        "                    verbose=1,\n",
        "                    random_state=RANDOM_STATE\n",
        "                )\n",
        "            else:\n",
        "                print(\"\\nPerforming grid search...\")\n",
        "                search = GridSearchCV(\n",
        "                    rf,\n",
        "                    param_grid,\n",
        "                    cv=3,\n",
        "                    scoring='neg_mean_squared_error',\n",
        "                    n_jobs=N_JOBS,\n",
        "                    verbose=1\n",
        "                )\n",
        "            \n",
        "            search.fit(X_train, y_train)\n",
        "            \n",
        "            best_model = search.best_estimator_\n",
        "            best_params = search.best_params_\n",
        "            best_score = -search.best_score_  # Convert to positive MSE\n",
        "            \n",
        "            print(f\"\\n✓ Search completed\")\n",
        "            print(f\"  Best parameters: {best_params}\")\n",
        "            print(f\"  Best CV MSE: {best_score:.6f}\")\n",
        "            \n",
        "        else:\n",
        "            print(\"\\nTraining with default parameters...\")\n",
        "            best_model = RandomForestRegressor(\n",
        "                n_estimators=100,\n",
        "                max_depth=20,\n",
        "                random_state=RANDOM_STATE,\n",
        "                n_jobs=N_JOBS\n",
        "            )\n",
        "            best_model.fit(X_train, y_train)\n",
        "            best_params = {\n",
        "                'n_estimators': 100,\n",
        "                'max_depth': 20,\n",
        "                'min_samples_split': 2,\n",
        "                'min_samples_leaf': 1,\n",
        "                'max_features': 'sqrt'\n",
        "            }\n",
        "            best_score = None\n",
        "        \n",
        "        training_time = time.time() - start_time\n",
        "        \n",
        "        # Store model\n",
        "        rf_models[target] = best_model\n",
        "        \n",
        "        # Save model\n",
        "        print(\"\\nSaving model...\")\n",
        "        model_path = save_model(\n",
        "            best_model,\n",
        "            model_name='random_forest',\n",
        "            target=target,\n",
        "            config={\n",
        "                'params': best_params,\n",
        "                'n_features': len(X_train.columns),\n",
        "                'features': list(X_train.columns)\n",
        "            },\n",
        "            models_dir='models'\n",
        "        )\n",
        "        \n",
        "        training_results[target] = {\n",
        "            'params': best_params,\n",
        "            'n_features': len(X_train.columns),\n",
        "            'n_samples': len(y_train),\n",
        "            'training_time': training_time,\n",
        "            'cv_mse': best_score,\n",
        "            'model_path': model_path\n",
        "        }\n",
        "        \n",
        "        print(f\"✓ Training completed in {training_time:.2f}s\")\n",
        "        \n",
        "        # Feature importance\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': X_train.columns,\n",
        "            'importance': best_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        print(f\"\\nTop 5 Important Features:\")\n",
        "        for idx, row in feature_importance.head(5).iterrows():\n",
        "            print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Training failed: {str(e)}\")\n",
        "        training_results[target] = {'error': str(e), 'success': False}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Training completed and models saved\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rf_rolling_forecast(model, X_test, y_test, horizon=20):\n",
        "    \"\"\"\n",
        "    Rolling forecast for Random Forest\n",
        "    Predict 'horizon' steps ahead at each time point\n",
        "    \"\"\"\n",
        "    n_test = len(y_test)\n",
        "    predictions = []\n",
        "    \n",
        "    # Can only forecast where we have enough future data\n",
        "    n_forecast_points = n_test - horizon + 1\n",
        "    \n",
        "    print(f\"  Forecasting {n_forecast_points} points with horizon={horizon}\")\n",
        "    \n",
        "    for i in range(n_forecast_points):\n",
        "        # Use features at time t to predict value at time t+horizon\n",
        "        X_current = X_test.iloc[i:i+1]\n",
        "        pred = model.predict(X_current)[0]\n",
        "        predictions.append(pred)\n",
        "        \n",
        "        if (i + 1) % 5000 == 0:\n",
        "            print(f\"    Progress: {i+1}/{n_forecast_points}\")\n",
        "    \n",
        "    # Align actual values\n",
        "    predictions = np.array(predictions)\n",
        "    actual = y_test.iloc[horizon:horizon+n_forecast_points].values\n",
        "    \n",
        "    return predictions, actual\n",
        "\n",
        "# Perform forecasting\n",
        "print(\"=\"*80)\n",
        "print(f\"MULTI-STEP FORECASTING (Horizon: {FORECAST_HORIZON} steps = 10 minutes)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "forecast_results = {}\n",
        "\n",
        "for target in target_vars:\n",
        "    if target not in rf_models:\n",
        "        print(f\"\\n✗ Skipping {target} - model not trained\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Target: {target}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    model = rf_models[target]\n",
        "    X_test = datasets[target]['X_test']\n",
        "    y_test = datasets[target]['y_test']\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        predictions, actual = rf_rolling_forecast(model, X_test, y_test, FORECAST_HORIZON)\n",
        "        \n",
        "        forecast_time = time.time() - start_time\n",
        "        \n",
        "        forecast_results[target] = {\n",
        "            'predictions': predictions,\n",
        "            'actual': actual,\n",
        "            'n_predictions': len(predictions),\n",
        "            'forecast_time': forecast_time,\n",
        "            'horizon': FORECAST_HORIZON\n",
        "        }\n",
        "        \n",
        "        print(f\"✓ Completed in {forecast_time:.2f}s\")\n",
        "        print(f\"  Predictions: {len(predictions):,}\")\n",
        "        print(f\"  Avg time: {forecast_time/len(predictions)*1000:.2f}ms per forecast\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Forecasting failed: {str(e)}\")\n",
        "        forecast_results[target] = {'error': str(e), 'success': False}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Forecasting completed\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "print(\"=\"*80)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "evaluation_results = {}\n",
        "\n",
        "for target in target_vars:\n",
        "    if target not in forecast_results or 'predictions' not in forecast_results[target]:\n",
        "        print(f\"\\n✗ Skipping {target} - no predictions\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Target: {target}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    y_true = forecast_results[target]['actual']\n",
        "    y_pred = forecast_results[target]['predictions']\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(y_true, y_pred)\n",
        "    evaluation_results[target] = metrics\n",
        "    \n",
        "    # Print formatted metrics\n",
        "    print_metrics(metrics, target)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Evaluation completed\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot feature importance for all targets\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('Random Forest: Feature Importance', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, target in enumerate(target_vars):\n",
        "    if target not in rf_models:\n",
        "        continue\n",
        "    \n",
        "    model = rf_models[target]\n",
        "    X_train = datasets[target]['X_train']\n",
        "    \n",
        "    # Get feature importance\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=True)\n",
        "    \n",
        "    # Plot\n",
        "    importance_df.plot(kind='barh', x='feature', y='importance', ax=axes[idx], legend=False)\n",
        "    axes[idx].set_title(f'{target}')\n",
        "    axes[idx].set_xlabel('Importance')\n",
        "    axes[idx].set_ylabel('')\n",
        "    axes[idx].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot predictions vs actual\n",
        "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
        "fig.suptitle('Random Forest: Predictions vs Actual (10-minute horizon)', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, target in enumerate(target_vars):\n",
        "    if target not in forecast_results or 'predictions' not in forecast_results[target]:\n",
        "        continue\n",
        "    \n",
        "    y_true = forecast_results[target]['actual']\n",
        "    y_pred = forecast_results[target]['predictions']\n",
        "    \n",
        "    # Plot first 500 points\n",
        "    n_plot = min(500, len(y_true))\n",
        "    \n",
        "    axes[idx].plot(y_true[:n_plot], label='Actual', alpha=0.7, linewidth=1.5)\n",
        "    axes[idx].plot(y_pred[:n_plot], label='Predicted', alpha=0.7, linewidth=1.5)\n",
        "    axes[idx].set_title(f'{target} - MAE: {evaluation_results[target][\"mae\"]:.4f}, R²: {evaluation_results[target][\"r2\"]:.4f}')\n",
        "    axes[idx].set_xlabel('Time Step')\n",
        "    axes[idx].set_ylabel('Normalized Value')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('Random Forest: Predicted vs Actual', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, target in enumerate(target_vars):\n",
        "    if target not in forecast_results or 'predictions' not in forecast_results[target]:\n",
        "        continue\n",
        "    \n",
        "    y_true = forecast_results[target]['actual']\n",
        "    y_pred = forecast_results[target]['predictions']\n",
        "    \n",
        "    axes[idx].scatter(y_true, y_pred, alpha=0.3, s=10)\n",
        "    \n",
        "    # Perfect prediction line\n",
        "    min_val = min(y_true.min(), y_pred.min())\n",
        "    max_val = max(y_true.max(), y_pred.max())\n",
        "    axes[idx].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect')\n",
        "    \n",
        "    axes[idx].set_title(f'{target}\\\\nR² = {evaluation_results[target][\"r2\"]:.4f}')\n",
        "    axes[idx].set_xlabel('Actual')\n",
        "    axes[idx].set_ylabel('Predicted')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics comparison\n",
        "metrics_df = pd.DataFrame(evaluation_results).T\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Random Forest Performance Metrics', fontsize=16, fontweight='bold')\n",
        "\n",
        "metrics_df['mae'].plot(kind='bar', ax=axes[0, 0], color='skyblue')\n",
        "axes[0, 0].set_title('MAE')\n",
        "axes[0, 0].set_ylabel('MAE')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "metrics_df['rmse'].plot(kind='bar', ax=axes[0, 1], color='lightcoral')\n",
        "axes[0, 1].set_title('RMSE')\n",
        "axes[0, 1].set_ylabel('RMSE')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "metrics_df['mape'].plot(kind='bar', ax=axes[1, 0], color='lightgreen')\n",
        "axes[1, 0].set_title('MAPE')\n",
        "axes[1, 0].set_ylabel('MAPE (%)')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "metrics_df['r2'].plot(kind='bar', ax=axes[1, 1], color='plum')\n",
        "axes[1, 1].set_title('R² Score')\n",
        "axes[1, 1].set_ylabel('R²')\n",
        "axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile results\n",
        "final_results = {\n",
        "    'model': 'Random Forest',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'forecast_horizon': FORECAST_HORIZON,\n",
        "    'forecast_horizon_minutes': FORECAST_HORIZON * 0.5,\n",
        "    'targets': {}\n",
        "}\n",
        "\n",
        "for target in target_vars:\n",
        "    if target not in evaluation_results:\n",
        "        continue\n",
        "    \n",
        "    final_results['targets'][target] = {\n",
        "        'model_config': {\n",
        "            'params': training_results[target]['params'],\n",
        "            'n_features': training_results[target]['n_features']\n",
        "        },\n",
        "        'training': {\n",
        "            'samples': training_results[target]['n_samples'],\n",
        "            'time_seconds': training_results[target]['training_time'],\n",
        "            'cv_mse': training_results[target].get('cv_mse'),\n",
        "            'model_path': training_results[target]['model_path']\n",
        "        },\n",
        "        'forecasting': {\n",
        "            'n_predictions': forecast_results[target]['n_predictions'],\n",
        "            'time_seconds': forecast_results[target]['forecast_time']\n",
        "        },\n",
        "        'metrics': evaluation_results[target]\n",
        "    }\n",
        "\n",
        "# Save results\n",
        "output_file = save_results(final_results, 'results_random_forest.json')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Model: Random Forest\")\n",
        "print(f\"Forecast horizon: {FORECAST_HORIZON} steps ({FORECAST_HORIZON*0.5:.1f} min)\")\n",
        "print(f\"Results saved to: {output_file}\")\n",
        "print(f\"Models saved in: models/\")\n",
        "\n",
        "print(f\"\\nMetrics Overview:\")\n",
        "for target in target_vars:\n",
        "    if target in final_results['targets']:\n",
        "        metrics = final_results['targets'][target]['metrics']\n",
        "        print(f\"\\n  {target}:\")\n",
        "        print(f\"    MAE:  {metrics['mae']:.6f}\")\n",
        "        print(f\"    RMSE: {metrics['rmse']:.6f}\")\n",
        "        print(f\"    R²:   {metrics['r2']:.6f}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test with Different Horizons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with multiple horizons\n",
        "HORIZONS_TO_TEST = [10, 20, 40, 60]  # 5, 10, 20, 30 minutes\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING MULTIPLE HORIZONS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Horizons: {HORIZONS_TO_TEST} steps = {[h*0.5 for h in HORIZONS_TO_TEST]} minutes\")\n",
        "print()\n",
        "\n",
        "horizon_comparison = {}\n",
        "\n",
        "for target in target_vars:\n",
        "    if target not in rf_models:\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Target: {target}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    model = rf_models[target]\n",
        "    X_test = datasets[target]['X_test']\n",
        "    y_test = datasets[target]['y_test']\n",
        "    \n",
        "    horizon_comparison[target] = {}\n",
        "    \n",
        "    for horizon in HORIZONS_TO_TEST:\n",
        "        print(f\"  Horizon: {horizon} steps ({horizon*0.5:.1f} min)... \", end='')\n",
        "        \n",
        "        try:\n",
        "            predictions, actual = rf_rolling_forecast(model, X_test, y_test, horizon)\n",
        "            metrics = calculate_metrics(actual, predictions)\n",
        "            \n",
        "            horizon_comparison[target][f\"h{horizon}\"] = {\n",
        "                'horizon': horizon,\n",
        "                'horizon_minutes': horizon * 0.5,\n",
        "                'n_predictions': len(predictions),\n",
        "                'metrics': metrics\n",
        "            }\n",
        "            \n",
        "            print(f\"MAE={metrics['mae']:.4f}, R²={metrics['r2']:.4f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Failed: {str(e)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ Horizon testing completed\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison table\n",
        "print(\"\\nHORIZON COMPARISON:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for target in target_vars:\n",
        "    if target not in horizon_comparison:\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{target.upper()}:\")\n",
        "    print(f\"{'Horizon':>10} {'Minutes':>10} {'MAE':>12} {'RMSE':>12} {'R²':>12}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for h_key in sorted(horizon_comparison[target].keys(),\n",
        "                       key=lambda x: horizon_comparison[target][x]['horizon']):\n",
        "        h = horizon_comparison[target][h_key]\n",
        "        print(f\"{h['horizon']:>10} {h['horizon_minutes']:>10.1f} \"\n",
        "              f\"{h['metrics']['mae']:>12.6f} {h['metrics']['rmse']:>12.6f} \"\n",
        "              f\"{h['metrics']['r2']:>12.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize performance vs horizon\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "fig.suptitle('Random Forest: Performance vs Forecast Horizon', fontsize=16, fontweight='bold')\n",
        "\n",
        "metrics_to_plot = ['mae', 'rmse', 'mape', 'r2']\n",
        "titles = ['MAE', 'RMSE', 'MAPE (%)', 'R²']\n",
        "colors_list = ['skyblue', 'lightcoral', 'lightgreen']\n",
        "\n",
        "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    \n",
        "    for cidx, target in enumerate(target_vars):\n",
        "        if target not in horizon_comparison:\n",
        "            continue\n",
        "        \n",
        "        horizons = []\n",
        "        values = []\n",
        "        \n",
        "        for h_key in sorted(horizon_comparison[target].keys(),\n",
        "                           key=lambda x: horizon_comparison[target][x]['horizon']):\n",
        "            h = horizon_comparison[target][h_key]\n",
        "            horizons.append(h['horizon_minutes'])\n",
        "            values.append(h['metrics'][metric])\n",
        "        \n",
        "        ax.plot(horizons, values, marker='o', linewidth=2,\n",
        "               label=target, alpha=0.7, color=colors_list[cidx])\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Horizon (minutes)')\n",
        "    ax.set_ylabel(title)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    if metric == 'r2':\n",
        "        ax.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save horizon comparison\n",
        "horizon_results = {\n",
        "    'model': 'Random Forest',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'horizons_tested': HORIZONS_TO_TEST,\n",
        "    'targets': horizon_comparison\n",
        "}\n",
        "\n",
        "horizon_file = save_results(horizon_results, 'results_random_forest_horizon_comparison.json')\n",
        "\n",
        "print(\"\\nBest horizon by MAE:\")\n",
        "for target in target_vars:\n",
        "    if target in horizon_comparison:\n",
        "        best = min(horizon_comparison[target].items(),\n",
        "                  key=lambda x: x[1]['metrics']['mae'])\n",
        "        print(f\"  {target}: {best[1]['horizon']} steps ({best[1]['horizon_minutes']:.1f} min) \"\n",
        "              f\"- MAE: {best[1]['metrics']['mae']:.6f}\")\n",
        "\n",
        "print(f\"\\n✓ Horizon comparison saved to: {horizon_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Completed:\n",
        "\n",
        "1. ✅ **Data Loading**: Loaded preprocessed train/test data\n",
        "2. ✅ **Hyperparameter Tuning**: Grid/Randomized search for optimal parameters\n",
        "3. ✅ **Model Training**: Random Forest with optimal hyperparameters\n",
        "4. ✅ **Model Saving**: Saved to `models/` directory\n",
        "5. ✅ **Feature Importance**: Analyzed most important features\n",
        "6. ✅ **Forecasting**: 10-minute ahead predictions (20 steps)\n",
        "7. ✅ **Evaluation**: MAE, RMSE, MAPE, R² metrics\n",
        "8. ✅ **Multi-Horizon Testing**: Tested with 5, 10, 20, 30 minute horizons\n",
        "9. ✅ **Visualization**: Performance comparisons\n",
        "10. ✅ **Results Saved**: JSON files for comparison\n",
        "\n",
        "### Output Files:\n",
        "\n",
        "- `models/random_forest_[target]_[timestamp].pkl`: Trained models\n",
        "- `results_random_forest.json`: Main results (20-step horizon)\n",
        "- `results_random_forest_horizon_comparison.json`: Multi-horizon comparison\n",
        "\n",
        "### How to Load and Test:\n",
        "\n",
        "```python\n",
        "from model_utils import load_model, calculate_metrics\n",
        "\n",
        "# Load saved model\n",
        "model, metadata = load_model('models/random_forest_memory_usage_pct_xxx.pkl')\n",
        "\n",
        "# Predict with custom horizon\n",
        "predictions, actual = rf_rolling_forecast(model, X_test, y_test, horizon=120)\n",
        "\n",
        "# Evaluate\n",
        "metrics = calculate_metrics(actual, predictions)\n",
        "print(f\"R²: {metrics['r2']:.4f}\")\n",
        "```\n",
        "\n",
        "### Model Comparison:\n",
        "\n",
        "Compare all three models:\n",
        "```python\n",
        "from model_utils import compare_results\n",
        "\n",
        "comparison = compare_results([\n",
        "    'results_arimax.json',\n",
        "    'results_svr.json',\n",
        "    'results_random_forest.json'\n",
        "])\n",
        "print(comparison)\n",
        "```\n",
        "\n",
        "### Advantages of Random Forest:\n",
        "\n",
        "- ✅ Handles non-linear relationships\n",
        "- ✅ Feature importance analysis\n",
        "- ✅ Robust to outliers\n",
        "- ✅ No assumptions about data distribution\n",
        "- ✅ Fast prediction after training\n",
        "\n",
        "---\n",
        "\n",
        "**All Random Forest models and results saved successfully!**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
